# 5 EXPERIMENTS

## minimal time lagsとは？

関連する入力信号とそれに対応する教師信号（ターゲット出力）との間の最小の時間遅延を指します。具体的には、以下のような意味があります。

- 1. 関連する入力信号と教師信号

        入力信号: モデルが受け取るデータや情報。

        教師信号: モデルがその入力データに基づいて予測または生成するべき正解データ。

- 2. 時間ラグ

        時間ラグ: 入力信号が発生してから、その信号に基づいて対応する教師信号が出現するまでの時間的な遅延。

- 3. 最小時間ラグ

        最小時間ラグ（minimal time lag）: 訓練データにおいて、すべての関連する入力信号と教師信号の間に存在する最も短い遅延。この「最小時間ラグ」が長いということは、関連する入力と出力の間にある程度の時間の間隔が常に存在することを意味します。


例えば、音声認識タスクを考えてみましょう。ここでは、以下のような場合が考えられます。

(1)短い時間ラグ: 「音声入力A」が発生してすぐに「対応するテキストB」が出現する。

(2)長い時間ラグ: 「音声入力A」が発生してから、しばらく時間が経過して「対応するテキストB」が出現する。

## Introduction

新しい長時間ラグアルゴリズムの品質を示すために適したタスクはどのようなものでしょうか？まず、関連する入力信号と対応する教師信号の間のminimal time lagsが、すべての訓練シーケンスに対して長いものでなければなりません。実際、多くの以前のリカレントネットワークアルゴリズムは、非常に短い訓練シーケンスから非常に長いテストシーケンスに一般化することが時々あります。

例として、Pollack (1991) を参照してください。しかし、実際の長い時間ラグ問題は、訓練セットに短い時間ラグの例を含みません。例えば、Elmanネットワーク、BPTT、オフラインRTRL、オンラインRTRLなどは、実際の長い時間ラグ問題ではひどく失敗します。例として、Hochreiter (1991) や Mozer (1992) を参照してください。もう一つの重要な要件は、タスクが十分に複雑であり、ランダムな重み推測のような単純な戦略では迅速に解決できないことです。

## ランダムな重み推測は、多くの長時間ラグアルゴリズムを上回ることがある

最近、私たちは（SchmidhuberとHochreiter 1996、HochreiterとSchmidhuber 1996、1997）多くの以前の研究で使用された長時間ラグタスクが、提案されたアルゴリズムよりも単純なランダム重み推測によってより迅速に解決できることを発見しました。例えば、ランダム重み推測は、BengioとFrasconiの「パリティ問題」（1994）の変種を、Bengioら（1994）およびBengioとFrasconi（1994）がテストした7つの方法よりもはるかに速く解決しました。同様に、MillerとGilesのいくつかの問題（1993）についても同様です。もちろん、これはランダム重み推測が良いアルゴリズムであることを意味するわけではありません。単に、以前使用されていたいくつかの問題が、提案されたアルゴリズムの質を示すのに非常に適しているわけではないことを意味します。

## "Solutions to most of our tasks are sparse in weight space." 

ほとんどのタスクに対する解決策が重み空間で疎であるということです。これは以下のような点を意味しています。

1. 重みの疎性：
疎であるとは、重みの多くがゼロに近いか、非常に小さい値を持つことを指します。つまり、全ての重みが重要なわけではなく、ネットワークの出力に影響を与えるのは少数の重要な重みだけであるということです。

2. 高次元空間：
重み空間とは、ネットワークの全ての重みを含む多次元の空間を指します。各重みがこの空間の1つの次元に対応します。疎な重み空間というのは、この多次元空間で多くの次元がゼロまたはほとんどゼロの値を持つことを意味します。

3. 学習の困難さ：
疎な重み空間では、適切な重みのセットを見つけることが困難になることがあります。特に、ランダムな重み推測では適切な解を見つけることが難しいです。これは、適切な重みのセットが非常に少ないためです。

4. 具体例：
例えば、あるタスクに対して最適な重みのセットが1000次元の重み空間の中で非常に少数しか存在しない場合、そのタスクの解決策は疎な重み空間に存在すると言えます。つまり、999次元の重みがゼロまたは非常に小さく、重要なのは1次元の重みだけであるような状況です。

5. まとめ：
"Solutions to most of our tasks are sparse in weight space." とは、ほとんどのタスクに対する解決策が、重み空間において多くの重みがゼロまたはほとんどゼロであることを指しています。これにより、適切な重みセットを見つけることが難しくなり、ランダムな重み推測が不適切である理由を説明しています。



## 実験1から6に共通する点

すべての実験（実験1を除く）は長い最小時間ラグを伴います。学習を促進するための短い時間ラグの訓練例はありません。

ほとんどのタスクの解決策は重み空間で疎です。

それらは多くのパラメータ/入力または高い重み精度を必要とするため、ランダムな重み推測が実行不可能になります。

常にオンライン学習（バッチ学習ではなく）を使用し、活性化関数としてロジスティックシグモイドを使用します。実験1および2の初期重みは[0.2; 0.2]の範囲で選ばれ、他の実験では[0.1; 0.1]の範囲で選ばれます。

訓練シーケンスは、各タスクの説明に従ってランダムに生成されます。付録A1の表記からわずかに逸脱して、各入力シーケンスの各離散時間ステップには、次の3つの処理ステップが含まれます：

1. 現在の入力を使用して入力ユニットを設定する。

2. 隠れユニット（入力ゲート、出力ゲート、メモリセルを含む）の活性化を計算する。

3. 出力ユニットの活性化を計算する。実験1、2a、および2bを除き、シーケンス要素はオンラインでランダムに生成され、誤差信号はシーケンスの終わりでのみ生成されます。各入力シーケンスが処理された後、ネットの活性化はリセットされます。

勾配降下法で訓練されたリカレントネットワークとの比較では、RTRL（リアルタイムリカレント学習）の結果のみを示します。ただし、比較2aではBPTT（時間逆伝播法）も含まれます。

しかし、切り詰められていないBPTT（例えば、WilliamsとPeng 1990を参照）は、オフラインRTRLと全く同じ勾配を計算することに注意してください。

長時間ラグ問題では、オフラインRTRL（またはBPTT）とオンライン版のRTRL（活性化リセットなし、オンラインでの重み変更）は、ほぼ同一の否定的な結果をもたらします（Hochreiter 1991による追加シミュレーションで確認されました。Mozer 1992も参照）。これは、オフラインRTRL、オンラインRTRL、およびフルBPTTがすべて指数関数的な誤差減衰にひどく悩まされるためです。

LSTMアーキテクチャはかなり任意に選択されています。

与えられた問題の複雑さについて何もわからない場合、より体系的なアプローチは次のようになります：1つのメモリセルからなる非常に小さなネットから始めます。

これが機能しない場合は、2つのセルを試すなど、あるいは、逐次ネットワーク構築を使用します（例えば、Fahlman 1991）。
## Outline of experiment

### Experiment 1 

実験1は、リカレントネットワークの標準ベンチマークテストであるthe embedded Reber grammarに焦点を当てています。これは短い時間ラグを持つ訓練シーケンスを許容するため、長い時間ラグの問題ではありません。

これを含める理由は、(1) LSTMの出力ゲートが真に有益であることを示す良い例を提供するため、および(2) 多くの著者によって使用されてきたリカレントネットワークの人気のベンチマークであるためです。

少なくとも、従来のBPTTやRTRLが完全に失敗しない実験を1つは含めたいと考えています（ただし、LSTMは明らかにそれらを上回ります）。埋め込みReber文法の最小時間ラグは、従来のアルゴリズムでそれらを橋渡しすることがまだ可能であるという意味で境界ケースを表しています。ほんの少しだけ長い最小時間ラグでも、これをほぼ不可能にします。しかし、私たちの論文でより興味深いタスクは、RTRLやBPTTなどが全く解決できないものです。

### Experiment 2

実験2は、多数の入力記号が含まれるノイズのないおよびノイズの多いシーケンスに焦点を当てています。

これらの記号は、重要なものから注意をそらすように設計されています。

最も難しいタスク（タスク2c）は、ランダムな位置に数百の妨害記号が含まれ、最小時間ラグが1000ステップに及びます。

LSTMはこれを解決しますが、BPTTとRTRLは既に10ステップの最小時間ラグの場合に失敗します（例えば、Hochreiter 1991およびMozer 1992も参照）。

このため、RTRLとBPTTは残りのより複雑な実験では省略されており、これらの実験はすべてはるかに長い時間ラグを含みます。

### Experiment 3

実験3は、同じ入力ラインにノイズと信号がある長時間ラグの問題に取り組みます。

実験3a/3bは、Bengioらの1994年の「2シーケンス問題」に焦点を当てています。

この問題は実際にはランダムな重み推測によって迅速に解決できるため、より難易度の高い2シーケンス問題（3c）も含めています。
これは、入力を与えられた状態で、ノイズの多いターゲットの実数値の条件付き期待値を学習する必要があります。

### Experiments 4 and 5 

実験4および5は、分散された連続値の入力表現を含み、非常に長い期間にわたって正確な実数値を記憶することを学習する必要があります。

関連する入力信号は、入力シーケンス内の非常に異なる位置で発生する可能性があります。ここでも最小時間ラグは数百ステップに及びます。このようなタスクは他のリカレントネットアルゴリズムでは決して解決されていません。

### Experiment 6

実験6は、他のリカレントネットアルゴリズムによっても解決されていない、異なる複雑なタイプのタスクに関するものです。ここでも、関連する入力信号は入力シーケンスの非常に異なる位置で発生する可能性があります。この実験は、LSTMが広く分散された入力の時間順序によって伝えられる情報を抽出できることを示しています。


## 5.1 EXPERIMENT 1: EMBEDDED REBER GRAMMAR

最初のタスクは、「embedded Reber grammar」を学習することです。

これについては、Smith and Zipser (1989)、Cleeremans et al. (1989)、Fahlman (1991) などで取り上げられています。

このタスクでは、最小9ステップの短い時間ラグを持つ訓練シーケンスを使用できるため、長時間ラグの問題ではありません。

このタスクを含めた理由は2つあります：(1) 多くの著者によって使用されている人気のあるリカレントネットワークのベンチマークであるため、RTRLやBPTTが完全には失敗しない実験を少なくとも1つ含めたかったこと、および(2) 出力ゲートがどれほど有益であるかを示す良い例であることです。

![](https://private-user-images.githubusercontent.com/82156802/354876561-d940ba52-bd23-49df-bbc2-153ee11ace16.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjI3NDcyODgsIm5iZiI6MTcyMjc0Njk4OCwicGF0aCI6Ii84MjE1NjgwMi8zNTQ4NzY1NjEtZDk0MGJhNTItYmQyMy00OWRmLWJiYzItMTUzZWUxMWFjZTE2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA4MDQlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwODA0VDA0NDk0OFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWE1NjQyZDdlMWIzYjcxMjkyYTNjYzUzYTBjYTk1ZDA3YTQ1ZTk2M2ZlZWZhMGNjYzdkNzZiNDljOWFhYmVlN2QmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.cNjF99IbZa9B66juQUVYXRhVhkbNnOs5msWYorjQJnE)


図3: Reber文法の遷移図


![](https://private-user-images.githubusercontent.com/82156802/354876595-341a02c4-817e-48b9-a164-833610693142.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjI3NDczNTksIm5iZiI6MTcyMjc0NzA1OSwicGF0aCI6Ii84MjE1NjgwMi8zNTQ4NzY1OTUtMzQxYTAyYzQtODE3ZS00OGI5LWExNjQtODMzNjEwNjkzMTQyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA4MDQlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwODA0VDA0NTA1OVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWY4NTFkZTAzN2JmOWRlYTRkMWExZDY3MjI0OGYxNDA2ZjIzNjBiNWJhOWM3NGZlZjExMGQ0ZDQ5YzViNGQ0MjMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.z6eF2dy7V2F4ST4_dDYdbKVjyHQ7xIcgtSK-LAhpmKQ)

図4: 埋め込みReber文法の遷移図。各ボックスはReber文法（図3参照）のコピーを表しています。

## 5.2 EXPERIMENT 2: NOISE-FREE AND NOISY SEQUEN

![](https://private-user-images.githubusercontent.com/82156802/354876917-7a5dba78-4cb1-4ffa-9b6f-665cc9e71b88.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjI3NDc2NDUsIm5iZiI6MTcyMjc0NzM0NSwicGF0aCI6Ii84MjE1NjgwMi8zNTQ4NzY5MTctN2E1ZGJhNzgtNGNiMS00ZmZhLTliNmYtNjY1Y2M5ZTcxYjg4LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA4MDQlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwODA0VDA0NTU0NVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTIyNjlhMjdlZjY5NDZhOGM1MjVmNDBhN2Q5ZjVhNGMzN2FiMjZhODA2NDcyZWUwMjNmNzJhM2UyYzFlNGRiMzkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0._fZNtoYkkt7yvZEcoxS5gd025-w23wioK8NZ1qVyVWY)