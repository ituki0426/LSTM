# 5 EXPERIMENTS

## minimal time lagsとは？

関連する入力信号とそれに対応する教師信号（ターゲット出力）との間の最小の時間遅延を指します。具体的には、以下のような意味があります。

- 1. 関連する入力信号と教師信号

        入力信号: モデルが受け取るデータや情報。

        教師信号: モデルがその入力データに基づいて予測または生成するべき正解データ。

- 2. 時間ラグ

        時間ラグ: 入力信号が発生してから、その信号に基づいて対応する教師信号が出現するまでの時間的な遅延。

- 3. 最小時間ラグ

        最小時間ラグ（minimal time lag）: 訓練データにおいて、すべての関連する入力信号と教師信号の間に存在する最も短い遅延。この「最小時間ラグ」が長いということは、関連する入力と出力の間にある程度の時間の間隔が常に存在することを意味します。


例えば、音声認識タスクを考えてみましょう。ここでは、以下のような場合が考えられます。

(1)短い時間ラグ: 「音声入力A」が発生してすぐに「対応するテキストB」が出現する。

(2)長い時間ラグ: 「音声入力A」が発生してから、しばらく時間が経過して「対応するテキストB」が出現する。

## Introduction

新しい長時間ラグアルゴリズムの品質を示すために適したタスクはどのようなものでしょうか？まず、関連する入力信号と対応する教師信号の間のminimal time lagsが、すべての訓練シーケンスに対して長いものでなければなりません。実際、多くの以前のリカレントネットワークアルゴリズムは、非常に短い訓練シーケンスから非常に長いテストシーケンスに一般化することが時々あります。

例として、Pollack (1991) を参照してください。しかし、実際の長い時間ラグ問題は、訓練セットに短い時間ラグの例を含みません。例えば、Elmanネットワーク、BPTT、オフラインRTRL、オンラインRTRLなどは、実際の長い時間ラグ問題ではひどく失敗します。例として、Hochreiter (1991) や Mozer (1992) を参照してください。もう一つの重要な要件は、タスクが十分に複雑であり、ランダムな重み推測のような単純な戦略では迅速に解決できないことです。

## ランダムな重み推測は、多くの長時間ラグアルゴリズムを上回ることがある

最近、私たちは（SchmidhuberとHochreiter 1996、HochreiterとSchmidhuber 1996、1997）多くの以前の研究で使用された長時間ラグタスクが、提案されたアルゴリズムよりも単純なランダム重み推測によってより迅速に解決できることを発見しました。例えば、ランダム重み推測は、BengioとFrasconiの「パリティ問題」（1994）の変種を、Bengioら（1994）およびBengioとFrasconi（1994）がテストした7つの方法よりもはるかに速く解決しました。同様に、MillerとGilesのいくつかの問題（1993）についても同様です。もちろん、これはランダム重み推測が良いアルゴリズムであることを意味するわけではありません。単に、以前使用されていたいくつかの問題が、提案されたアルゴリズムの質を示すのに非常に適しているわけではないことを意味します。

## "Solutions to most of our tasks are sparse in weight space." 

ほとんどのタスクに対する解決策が重み空間で疎であるということです。これは以下のような点を意味しています。

1. 重みの疎性：
疎であるとは、重みの多くがゼロに近いか、非常に小さい値を持つことを指します。つまり、全ての重みが重要なわけではなく、ネットワークの出力に影響を与えるのは少数の重要な重みだけであるということです。

2. 高次元空間：
重み空間とは、ネットワークの全ての重みを含む多次元の空間を指します。各重みがこの空間の1つの次元に対応します。疎な重み空間というのは、この多次元空間で多くの次元がゼロまたはほとんどゼロの値を持つことを意味します。

3. 学習の困難さ：
疎な重み空間では、適切な重みのセットを見つけることが困難になることがあります。特に、ランダムな重み推測では適切な解を見つけることが難しいです。これは、適切な重みのセットが非常に少ないためです。

4. 具体例：
例えば、あるタスクに対して最適な重みのセットが1000次元の重み空間の中で非常に少数しか存在しない場合、そのタスクの解決策は疎な重み空間に存在すると言えます。つまり、999次元の重みがゼロまたは非常に小さく、重要なのは1次元の重みだけであるような状況です。

5. まとめ：
"Solutions to most of our tasks are sparse in weight space." とは、ほとんどのタスクに対する解決策が、重み空間において多くの重みがゼロまたはほとんどゼロであることを指しています。これにより、適切な重みセットを見つけることが難しくなり、ランダムな重み推測が不適切である理由を説明しています。



## 実験1から6に共通する点

すべての実験（実験1を除く）は長い最小時間ラグを伴います。学習を促進するための短い時間ラグの訓練例はありません。

ほとんどのタスクの解決策は重み空間で疎です。

それらは多くのパラメータ/入力または高い重み精度を必要とするため、ランダムな重み推測が実行不可能になります。

常にオンライン学習（バッチ学習ではなく）を使用し、活性化関数としてロジスティックシグモイドを使用します。実験1および2の初期重みは[0.2; 0.2]の範囲で選ばれ、他の実験では[0.1; 0.1]の範囲で選ばれます。

訓練シーケンスは、各タスクの説明に従ってランダムに生成されます。付録A1の表記からわずかに逸脱して、各入力シーケンスの各離散時間ステップには、次の3つの処理ステップが含まれます：

1. 現在の入力を使用して入力ユニットを設定する。

2. 隠れユニット（入力ゲート、出力ゲート、メモリセルを含む）の活性化を計算する。

3. 出力ユニットの活性化を計算する。実験1、2a、および2bを除き、シーケンス要素はオンラインでランダムに生成され、誤差信号はシーケンスの終わりでのみ生成されます。各入力シーケンスが処理された後、ネットの活性化はリセットされます。

勾配降下法で訓練されたリカレントネットワークとの比較では、RTRL（リアルタイムリカレント学習）の結果のみを示します。ただし、比較2aではBPTT（時間逆伝播法）も含まれます。

しかし、切り詰められていないBPTT（例えば、WilliamsとPeng 1990を参照）は、オフラインRTRLと全く同じ勾配を計算することに注意してください。

長時間ラグ問題では、オフラインRTRL（またはBPTT）とオンライン版のRTRL（活性化リセットなし、オンラインでの重み変更）は、ほぼ同一の否定的な結果をもたらします（Hochreiter 1991による追加シミュレーションで確認されました。Mozer 1992も参照）。これは、オフラインRTRL、オンラインRTRL、およびフルBPTTがすべて指数関数的な誤差減衰にひどく悩まされるためです。

LSTMアーキテクチャはかなり任意に選択されています。

与えられた問題の複雑さについて何もわからない場合、より体系的なアプローチは次のようになります：1つのメモリセルからなる非常に小さなネットから始めます。

これが機能しない場合は、2つのセルを試すなど、あるいは、逐次ネットワーク構築を使用します（例えば、Fahlman 1991）。
## Outline of experiment

### Experiment 1 

実験1は、リカレントネットワークの標準ベンチマークテストであるthe embedded Reber grammarに焦点を当てています。これは短い時間ラグを持つ訓練シーケンスを許容するため、長い時間ラグの問題ではありません。

これを含める理由は、(1) LSTMの出力ゲートが真に有益であることを示す良い例を提供するため、および(2) 多くの著者によって使用されてきたリカレントネットワークの人気のベンチマークであるためです。

少なくとも、従来のBPTTやRTRLが完全に失敗しない実験を1つは含めたいと考えています（ただし、LSTMは明らかにそれらを上回ります）。埋め込みReber文法の最小時間ラグは、従来のアルゴリズムでそれらを橋渡しすることがまだ可能であるという意味で境界ケースを表しています。ほんの少しだけ長い最小時間ラグでも、これをほぼ不可能にします。しかし、私たちの論文でより興味深いタスクは、RTRLやBPTTなどが全く解決できないものです。

### Experiment 2

実験2は、多数の入力記号が含まれるノイズのないおよびノイズの多いシーケンスに焦点を当てています。

これらの記号は、重要なものから注意をそらすように設計されています。

最も難しいタスク（タスク2c）は、ランダムな位置に数百の妨害記号が含まれ、最小時間ラグが1000ステップに及びます。

LSTMはこれを解決しますが、BPTTとRTRLは既に10ステップの最小時間ラグの場合に失敗します（例えば、Hochreiter 1991およびMozer 1992も参照）。

このため、RTRLとBPTTは残りのより複雑な実験では省略されており、これらの実験はすべてはるかに長い時間ラグを含みます。

### Experiment 3

実験3は、同じ入力ラインにノイズと信号がある長時間ラグの問題に取り組みます。

実験3a/3bは、Bengioらの1994年の「2シーケンス問題」に焦点を当てています。

この問題は実際にはランダムな重み推測によって迅速に解決できるため、より難易度の高い2シーケンス問題（3c）も含めています。
これは、入力を与えられた状態で、ノイズの多いターゲットの実数値の条件付き期待値を学習する必要があります。

### Experiments 4 and 5 

実験4および5は、分散された連続値の入力表現を含み、非常に長い期間にわたって正確な実数値を記憶することを学習する必要があります。

関連する入力信号は、入力シーケンス内の非常に異なる位置で発生する可能性があります。ここでも最小時間ラグは数百ステップに及びます。このようなタスクは他のリカレントネットアルゴリズムでは決して解決されていません。

### Experiment 6

実験6は、他のリカレントネットアルゴリズムによっても解決されていない、異なる複雑なタイプのタスクに関するものです。ここでも、関連する入力信号は入力シーケンスの非常に異なる位置で発生する可能性があります。この実験は、LSTMが広く分散された入力の時間順序によって伝えられる情報を抽出できることを示しています。


## 5.1 EXPERIMENT 1: EMBEDDED REBER GRAMMAR

最初のタスクは、「embedded Reber grammar」を学習することです。

これについては、Smith and Zipser (1989)、Cleeremans et al. (1989)、Fahlman (1991) などで取り上げられています。

このタスクでは、最小9ステップの短い時間ラグを持つ訓練シーケンスを使用できるため、長時間ラグの問題ではありません。

このタスクを含めた理由は2つあります：(1) 多くの著者によって使用されている人気のあるリカレントネットワークのベンチマークであるため、RTRLやBPTTが完全には失敗しない実験を少なくとも1つ含めたかったこと、および(2) 出力ゲートがどれほど有益であるかを示す良い例であることです。

![](https://private-user-images.githubusercontent.com/82156802/354876561-d940ba52-bd23-49df-bbc2-153ee11ace16.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjI3NDcyODgsIm5iZiI6MTcyMjc0Njk4OCwicGF0aCI6Ii84MjE1NjgwMi8zNTQ4NzY1NjEtZDk0MGJhNTItYmQyMy00OWRmLWJiYzItMTUzZWUxMWFjZTE2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA4MDQlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwODA0VDA0NDk0OFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWE1NjQyZDdlMWIzYjcxMjkyYTNjYzUzYTBjYTk1ZDA3YTQ1ZTk2M2ZlZWZhMGNjYzdkNzZiNDljOWFhYmVlN2QmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.cNjF99IbZa9B66juQUVYXRhVhkbNnOs5msWYorjQJnE)


> 図3: Reber文法の遷移図


![](https://private-user-images.githubusercontent.com/82156802/354876595-341a02c4-817e-48b9-a164-833610693142.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjI3NDczNTksIm5iZiI6MTcyMjc0NzA1OSwicGF0aCI6Ii84MjE1NjgwMi8zNTQ4NzY1OTUtMzQxYTAyYzQtODE3ZS00OGI5LWExNjQtODMzNjEwNjkzMTQyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA4MDQlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwODA0VDA0NTA1OVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWY4NTFkZTAzN2JmOWRlYTRkMWExZDY3MjI0OGYxNDA2ZjIzNjBiNWJhOWM3NGZlZjExMGQ0ZDQ5YzViNGQ0MjMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.z6eF2dy7V2F4ST4_dDYdbKVjyHQ7xIcgtSK-LAhpmKQ)

> 図4: 埋め込みReber文法の遷移図。各ボックスはReber文法（図3参照）のコピーを表しています。

図4の有向グラフの左端のノードから、エッジに沿ってシンボル列を生成します。

シンボル列の生成は、（空の文字列から始めて）順にエッジをたどり、関連するシンボルを現在の文字列に追加していき、右端のノードに到達するまで続けます。

選択肢がある場合にはエッジはランダムに選ばれます（確率: 0.5）。

ネットのタスクは文字列を一文字ずつ読み取り、次のシンボルを常に予測することです（各時間ステップでエラー信号が発生します）。

最後のシンボルの前のシンボルを正しく予測するためには、ネットは2番目のシンボルを記憶していなければなりません。


### Comparison

LSTMを、Elmanのトレーニング手法でトレーニングされたElmanネット（ELM）（Cleeremansら1989の結果より）、Fahlmanの「Recurrent Cascade-Correlation」（RCC）（Fahlman 1991の結果より）、およびRTRL（SmithとZipser 1989の結果より、成功した試行のみをリストしたもの）と比較します。

SmithとZipserは、短いタイムラグの例の確率を増やすことでタスクを実際に簡単にしていることを述べておくべきです。

私たちはLSTMにはこれを行いませんでした。

### Training/Testing

我々は局所的な入力/出力表現を使用します（7つの入力ユニット、7つの出力ユニット）。Fahlmanに従い、256のトレーニング文字列と256の独立したテスト文字列を使用します。

トレーニングセットはランダムに生成され、トレーニングの例はトレーニングセットからランダムに選ばれます。

テストシーケンスもランダムに生成されますが、トレーニングセットで既に使用されたシーケンスはテストには使用されません。

文字列の提示後、すべての活性化状態はゼロに再初期化されます。

トライアルは、テストセットとトレーニングセットのすべてのシーケンスのすべての文字列シンボルが正しく予測された場合、つまり、次の可能性のあるシンボルに対応する出力ユニットが常に最も活性化されている場合に成功と見なされます。


### Architectures

RTRL、ELM、RCCのアーキテクチャについては、上記の参考文献に報告されています。

LSTMについては、3つ（または4つ）のメモリセルブロックを使用します。

各ブロックには2つ（または1つ）のメモリセルがあります。

出力層の入力接続はすべてメモリセルから来ています。

各メモリセルおよび各ゲートユニットは、すべてのメモリセルおよびゲートユニットから入力接続を受け取ります（隠れ層は完全に接続されており、接続性が少ない場合でも機能する可能性があります）。

入力層は、隠れ層のすべてのユニットに対して前方接続を持っています。

ゲートユニットにはバイアスがかかっています。これらのアーキテクチャパラメータは、少なくとも3つの入力信号を記憶するのに適しています（アーキテクチャ3-2と4-1は、両方のアーキテクチャで比較可能な重みの数を得るために使用されます：4-1では264、3-2では276）。

他のパラメータも適している可能性がありますが、すべてのシグモイド関数は出力範囲[0; 1]のロジスティック関数で、hの範囲は[1; 1]、gの範囲は[2; 2]です。

すべての重みは[0.2; 0.2]で初期化され、出力ゲートバイアスはそれぞれ-1、-2、-3で初期化されます（セクション4の問題と解決策（2）を参照）。

我々は学習率として0.1、0.2、および0を試しました。

### Results

私たちは、3つの異なるランダムに生成された訓練セットとテストセットのペアを使用します。

各ペアについて、異なる初期重みで10回の試行を行います。結果については5.2にある表1を参照してください（30回の試行の平均）。

他の方法とは異なり、LSTMは常にタスクを解決することを学習します。他のアプローチの不成功の試行を無視した場合でも、LSTMははるかに速く学習します。


### Importance of output gates

この実験は、出力ゲートが真に有益であることを示す良い例を提供しています。

最初の T または P を記憶することを学習する際に、元の Reber 文法のより容易に学習できる遷移を表現する活性化を乱さないようにする必要があります。

これは出力ゲートの役割です。

出力ゲートがなければ、私たちは速い学習を達成できませんでした。


## 5.2 EXPERIMENT 2: NOISE-FREE AND NOISY SEQUEN

### Task 2a: noise-free sequences with long time lag

p + 1 個の可能な入力記号があり、それぞれ 
$$
a_{1}, ......, a_{p-1}
$$

$$
a_{p} = x, a_{p+1} = y 
$$ 
と表されます。

$a_{i}$ は、i 番目の成分が 1（他のすべての成分が 0）である p + 1次元ベクトルで「局所的に」表現されます。

$p + 1$個の入力ユニットと$p + 1$個の出力ユニットを持つネットワークは、順次入力記号列を一度に一つずつ観察し、常に次の記号を予測しようとします - 誤差信号は毎回発生します。

「長時間遅延問題」を強調するために、非常に似た2つのシーケンスのみを含むトレーニングセットを使用します：

$(y, a_1, a_2, ..... , a_{p-1}; y)$ と $(x, a_1, a_2, ..... ,a_{p-1}, x)$

各シーケンスは確率0.5で選択されます。最終要素を予測するために、ネットは$p$タイムステップの間に最初の要素の表現を記憶することを学習する必要があります。

完全に再帰的なネットワークのための「リアルタイム再帰学習」(RTRL)、
「時間を通したバックプロパゲーション」(BPTT)、時折非常に成功する2ネット「ニューラルシーケンスチャンク」(CH, Schmidhuber 1992b)、そして我々の新しい方法(LSTM)を比較します。

すべての場合において、重みは[-0.2, 0.2]の範囲で初期化されます。計算時間が限られているため、トレーニングは500万回のシーケンス提示後に停止します。

![](https://private-user-images.githubusercontent.com/82156802/354938956-b1d272e2-b0a9-4a7f-80b4-3398ce53bcb9.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjI4MTc4NzQsIm5iZiI6MTcyMjgxNzU3NCwicGF0aCI6Ii84MjE1NjgwMi8zNTQ5Mzg5NTYtYjFkMjcyZTItYjBhOS00YTdmLTgwYjQtMzM5OGNlNTNiY2I5LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA4MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwODA1VDAwMjYxNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWRjMDMwYjNmNTk3ODE5NmJjZTQwMjVkZTY5NDc1ZDA5ZmRlZGQ5Mjk5OGY0Yjk1ZWQ4OWY3NTMzY2YyZDIyMDkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.KK1A4tSPEO841bIax-GsJk0pKqQvj91UFKz2Lr7ob8k)

> Table 1: 実験1: 埋め込みReber文法: 

> 成功した試行の割合と成功するまでのシーケンス提示回数についてのRTRL（Smith and Zipser 1989からの結果）、\Elmanの手法で訓練されたElmanネット"（Cleeremans et al. 1989からの結果）、\Recurrent Cascade-Correlation"（Fahlman 1991からの結果）、および我々の新しいアプローチ（LSTM）。

> 最初の4行の重みの数は推定値です | 対応する論文はすべての技術的詳細を提供していません。LSTMだけがほぼ常にタスクを解決することを学習します（150回の試行中2回のみ失敗）。

> 他のアプローチの不成功試行を無視した場合でも、LSTMははるかに速く学習します（最下行の必要なトレーニング例数は3,800から24,100の範囲で変動します）。


![](https://private-user-images.githubusercontent.com/82156802/354876917-7a5dba78-4cb1-4ffa-9b6f-665cc9e71b88.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjI3NDc2NDUsIm5iZiI6MTcyMjc0NzM0NSwicGF0aCI6Ii84MjE1NjgwMi8zNTQ4NzY5MTctN2E1ZGJhNzgtNGNiMS00ZmZhLTliNmYtNjY1Y2M5ZTcxYjg4LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA4MDQlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwODA0VDA0NTU0NVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTIyNjlhMjdlZjY5NDZhOGM1MjVmNDBhN2Q5ZjVhNGMzN2FiMjZhODA2NDcyZWUwMjNmNzJhM2UyYzFlNGRiMzkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0._fZNtoYkkt7yvZEcoxS5gd025-w23wioK8NZ1qVyVWY)
                         

> 表3：タスク2c：非常に長いminimal time lags  𝑞+1 と多くのノイズを伴うLSTM。 

> 𝑝は利用可能な妨害記号の数です（入力ユニットの数は 𝑝+4）𝑞/𝑝は、シーケンス内で特定の妨害記号が出現する予想回数です。

> 最右列は、LSTMが必要とする訓練シーケンスの数を示しています（BPTT、RTRL、および他の競合相手はこのタスクを解決する可能性がありません）。

> 妨害記号の数（および重み）を時間ラグに比例して増やすと、学習時間は非常にゆっくりと増加します。下のブロックは、妨害記号の頻度が増加したことによる予想される減速を示しています。

注意すべき点は、minnimal time lagsが 𝑞+1であることです。ネットワークは、長いテストシーケンスの分類を促進する短い訓練シーケンスを決して見ません。

### Results

すべてのテストされたペア（𝑝;𝑞）について20回の試行が行われました。

表3には、LSTMが成功を収めるために必要とする訓練シーケンスの平均数が記載されています（BPTTおよびRTRLは、最小時間ラグが1000ステップの非自明なタスクを解決する可能性がありません）。

### Scaling

スケーリング。表3は、入力記号（および重み）の数を時間ラグに比例して増やすと、学習時間が非常にゆっくりと増加することを示しています。

これは、他のどの方法とも共有されないLSTMのもう一つの驚くべき特性です。

実際、RTRLとBPTTは合理的にスケーリングするとは程遠く、代わりに指数関数的にスケーリングするように見え、時間ラグが10ステップを超えると非常に役に立たないように思われます。

### Distractor inuence

表3では、列の見出しにある 𝑞/𝑝が、妨害記号の予想頻度を示しています。

この頻度を増やすと学習速度が低下します。これは、頻繁に観察される入力記号によって引き起こされる重みの振動による影響です。

## 5.4 EXPERIMENT 4: ADDING PROBLEM

このセクションの難しいタスクは、他のリカレントネットアルゴリズムでは解決されたことがないタイプのものです。

これは、LSTMが分散された連続値表現を含む長いタイムラグ問題を解決できることを示しています。

### Task

### Architecture
## 5.5 EXPERIMENT 5: MULTIPLICATION PROBLEM


次の文を日本語に翻訳しました。

LSTMは前の小節で紹介したAdding Problemのようなタスクに対して少しバイアスがかかっていると言うかもしれません。

Adding Problemの解決策は、CECの組み込みの統合機能を利用することができます。

このCECの特性は欠点というよりもむしろ特徴と見なすことができるかもしれません（統合は現実世界で発生する多くのタスクの自然なサブタスクのように思われます）が、LSTMが本質的に非統合的な解決策を持つタスクも解決できるかどうかという疑問が生じます。

これをテストするために、問題を変更し、以前にマークされた入力の積（和ではなく）が最終目標となるようにします。

タスク。セクション5.4のタスクと同様ですが、各ペアの最初の要素は区間[0; 1]からランダムに選ばれた実数値です。入力シーケンスの最初のペアがマークされる場合には、稀なケースとして、X1を1.0に設定します。シーケンスの終了時の目標は積X1 × X2です。


## 5.6 EXPERIMENT 6: TEMPORAL ORDER

## 5.7 SUMMARY OF EXPERIMENTALCONDITION

この小節の2つの表は、実験1から6における最も重要なLSTMパラメータとアーキテクチャの詳細の概要を提供します。

単純な実験2aおよび2bの条件は、歴史的な理由により、他のより体系的な実験の条件とは若干異なります。

| Task | p  | lag | b  | s  | in | out | w   | c | ogb           | igb  | bias | h   | g   | α   |
|------|----|-----|----|----|----|-----|-----|---|---------------|------|------|-----|-----|-----|
| 1-1  | 9  | 9   | 4  | 1  | 7  | 7   | 264 | F | -1,-2,-3,-4  | r    | ga   | h1  | g2  | 0.1 |
| 1-2  | 9  | 9   | 3  | 2  | 7  | 7   | 276 | F | -1,-2,-3     | r    | ga   | h1  | g2  | 0.1 |
| 1-3  | 9  | 9   | 3  | 2  | 7  | 7   | 276 | F | -1,-2,-3     | r    | ga   | h1  | g2  | 0.2 |
| 1-4  | 9  | 9   | 4  | 1  | 7  | 7   | 264 | F | -1,-2,-3,-4  | r    | ga   | h1  | g2  | 0.5 |
| 1-5  | 9  | 9   | 3  | 2  | 7  | 7   | 276 | F | -1,-2,-3     | r    | ga   | h1  | g2  | 0.5 |
| 2a   | 100| 100 | 1  | 1  | 101| 101 | 10504| B | no og        | none | none | id  | g1  | 1.0 |
| 2b   | 100| 100 | 1  | 1  | 101| 101 | 10504| B | no og        | none | none | id  | g1  | 1.0 |
| 2c-1 | 50 | 50  | 2  | 1  | 54 | 2   | 364 | F | none         | none | none | h1  | g2  | 0.01|
| 2c-2 | 100| 100 | 2  | 1  | 104| 2   | 664 | F | none         | none | none | h1  | g2  | 0.01|
| 2c-3 | 200| 200 | 2  | 1  | 204| 2   | 1264| F | none         | none | none | h1  | g2  | 0.01|
| 2c-4 | 500| 500 | 2  | 1  | 504| 2   | 3064| F | none         | none | none | h1  | g2  | 0.01|
| 2c-5 |1000|1000 | 2  | 1  |1004| 2   | 6064| F | none         | none | none | h1  | g2  | 0.01|
| 2c-6 |1000|1000 | 2  | 1  | 504| 2   | 3064| F | none         | none | none | h1  | g2  | 0.01|
| 2c-7 |1000|1000 | 2  | 1  | 204| 2   | 1264| F | none         | none | none | h1  | g2  | 0.01|
| 2c-8 |1000|1000 | 2  | 1  | 104| 2   | 664 | F | none         | none | none | h1  | g2  | 0.01|
| 2c-9 |1000|1000 | 2  | 1  | 54 | 2   | 364 | F | none         | none | none | h1  | g2  | 0.01|
| 3a   | 100| 100 | 3  | 1  | 1  | 1   | 102 | F | -2,-4,-6     | -1,-3,-5 | b1 | h1  | g2  | 1.0 |
| 3b   | 100| 100 | 3  | 1  | 1  | 1   | 102 | F | -2,-4,-6     | -1,-3,-5 | b1 | h1  | g2  | 1.0 |
| 3c   | 100| 100 | 3  | 1  | 1  | 1   | 102 | F | -2,-4,-6     | -1,-3,-5 | b1 | h1  | g2  | 0.1 |
| 4-1  | 100| 50  | 2  | 2  | 2  | 1   | 93  | F | r            | -3,-6 | all | h1  | g2  | 0.5 |
| 4-2  | 500| 250 | 2  | 2  | 2  | 1   | 93  | F | r            | -3,-6 | all | h1  | g2  | 0.5 |
| 4-3  |1000| 500 | 2  | 2  | 2  | 1   | 93  | F | r            | -3,-6 | all | h1  | g2  | 0.5 |
| 5    | 100| 50  | 2  | 2  | 2  | 1   | 93  | F | r            | -2,-4 | all | h1  | g2  | 0.5 |
| 6a   | 100| 40  | 2  | 8  | 8  | 4   | 156 | F | r            | -2,-4 | all | h1  | g2  | 0.5 |
| 6b   | 100| 24  | 3  | 2  | 8  | 8   | 308 | F | r            | -2,-4,-6 | all | h1  | g2  | 0.5 |

> 表10: LSTMの実験条件の概要、Part1。

> 1列目: タスク番号。

> 2列目: 最小シーケンス長 p。

> 3列目: 最も最近の関連入力情報と教師信号の間の最小ステップ数。

> 4列目: セルブロックの数 b。

> 5列目: ブロックサイズ s。

> 6列目: 入力ユニットの数 in。

> 7列目: 出力ユニットの数 out。

> 8列目: 重みの数 w。

> 9列目: c は接続性を示します。「F」は「出力層がメモリセルから接続を受け取る。メモリセルとゲートユニットは入力ユニット、メモリセル、ゲートユニットから接続を受け取る」を意味します。「B」は「各層が下のすべての層から接続を受け取る」を意味します。

> 10列目: 初期出力ゲートバイアス ogb。「r」は「区間[0.1; 0.1]からランダムに選ばれる」を意味し、「no og」は「出力ゲートを使用しない」を意味します。

> 11列目: 初期入力ゲートバイアス igb（10列目を参照）。
 
> 12列目: どのユニットがバイアス重みを持つか。「b1」は「すべての隠れユニット」を意味し、「ga」は「ゲートユニットのみ」、「all」は「すべての非入力ユニット」を意味します。

> 13列目: 関数 h。「id」は恒等関数、「h1」は[2; 2]の範囲でのロジスティックシグモイド関数です。

> 14列目: ロジスティック関数 g。「g1」は[0; 1]の範囲でのシグモイド、「g2」は[1; 1]の範囲でのシグモイドです。

15列目: 学習率 α


| Task | select | interval  | test set size   | stopping criterion                           | success     |
|------|--------|-----------|-----------------|----------------------------------------------|-------------|
| 1    | t1     | [-0.2, 0.2] | 256             | training & test correctly pred.              | see text    |
| 2a   | t1     | [-0.2, 0.2] | no test set     | after 5 million exemplars                    | ABS(0.25)   |
| 2b   | t2     | [-0.2, 0.2] | 10000           | after 5 million exemplars                    | ABS(0.25)   |
| 2c   | t2     | [-0.2, 0.2] | 10000           | after 5 million exemplars                    | ABS(0.2)    |
| 3a   | t3     | [-0.1, 0.1] | 2560            | ST1 and ST2 (see text)                       | ABS(0.2)    |
| 3b   | t3     | [-0.1, 0.1] | 2560            | ST1 and ST2 (see text)                       | ABS(0.2)    |
| 3c   | t3     | [-0.1, 0.1] | 2560            | ST1 and ST2 (see text)                       | see text    |
| 4    | t3     | [-0.1, 0.1] | 2560            | ST3(0.01)                                    | ABS(0.04)   |
| 5    | t3     | [-0.1, 0.1] | 2560            | see text                                     | ABS(0.04)   |
| 6a   | t3     | [-0.1, 0.1] | 2560            | ST3(0.1)                                     | ABS(0.3)    |
| 6b   | t3     | [-0.1, 0.1] | 2560            | ST3(0.1)                                     | ABS(0.3)    |

> Table 11　:　LSTMの実験条件の概要、Part2

> 1列目: タスク番号。

> 2列目: トレーニング例の選択。「t1」は「トレーニングセットからランダムに選ばれる」、「t2」は「2つのクラスからランダムに選ばれる」、「t3」は「オンラインでランダムに生成される」を表します。

> 3列目: 重みの初期化区間。

> 4列目: テストセットのサイズ。
>
> 5列目: トレーニングの停止基準。「ST3(β)」は「平均トレーニングエラーがβ以下で、直近の2000シーケンスが正しく処理された場合」を表します。

> 6列目: 成功（正しい分類）基準。ABS(β)は「シーケンス終了時のすべての出力ユニットの絶対誤差がβ以下である場合」を表します。
