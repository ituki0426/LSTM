# LONG SHORT-TERM MEMORY


## Abstract

長時間の時間間隔で情報を記憶するためのリカレント逆伝播法（recurrent backpropagation）を学習するのには非常に長い時間がかかりますが、これは主に不十分で減衰するエラーバックフロー（error backflow）によるものです。

この問題について、まずHochreiterによる1991年の分析を簡単に振り返り、その後、「Long Short-Term Memory」（LSTM）という新しい効率的な勾配ベースの手法を導入して対処します。

勾配を無害な部分で切り詰めることにより、LSTMは特別なユニット内の「constant error carrousels」（一定のエラーカルーセル）を通じて一定のエラーフローを強制することで、1000を超える離散時間ステップの最小の時間ラグを橋渡しすることができます。

乗算ゲートユニットは、一定のエラーフローへのアクセスを開閉することを学習します。LSTMは空間と時間において局所的であり、各時間ステップおよび重みの計算複雑度はO(1)です。人工データを用いた実験では、局所的、分散的、実数値、およびノイズの多いパターン表現が含まれます。

RTRL、BPTT、Recurrent Cascade-Correlation、Elmanネット、およびNeural Sequence Chunkingとの比較において、LSTMはより多くの成功した実行をもたらし、学習速度もはるかに速いです。LSTMは、以前のリカレントネットワークアルゴリズムでは解決されなかった複雑な人工的な長時間ラグのタスクも解決します。

## 1 INTRODUCTION

リカレントネットワークは、原理的にはフィードバック接続を使用して、最近の入力イベントの表現をアクティベーションの形で記憶することができます（「短期記憶」、ゆっくり変化する重みによって具現化される「長期記憶」とは対照的です）。

これは、音声処理、非マルコフ制御、音楽作曲（例：Mozer 1992）など、多くのアプリケーションにとって潜在的に重要です。

しかしながら、短期記憶に何を入れるかを学習するために最も広く使用されているアルゴリズムは、時間がかかりすぎるか、全くうまく機能しないことが多く、特に入力と対応する教師信号の間の最小の時間ラグが長い場合には顕著です。

理論的には魅力的であるものの、既存の方法は、限られた時間ウィンドウを持つフィードフォワードネットのバックプロップに対して明確な実用的利点を提供しません。本論文では、この問題の分析をレビューし、解決策を提案します。

### The problem

従来の「時間逆伝播（Back-Propagation Through Time, BPTT）」(例：WilliamsとZipser 1992, Werbos 1988)や「リアルタイムリカレント学習（Real-Time Recurrent Learning, RTRL）」(例：RobinsonとFallside 1987)では、「時間的に逆方向に流れる」誤差信号は、(1) 発散するか (2) 消失する傾向があります。

逆伝播された誤差の時間的進化は重みの大きさに指数関数的に依存します (Hochreiter 1991)。ケース(1)では重みが振動し、ケース(2)では長い時間ラグを橋渡しするための学習に非常に多くの時間がかかるか、全く機能しません（セクション3を参照）。

### The remedy

本論文では、「Long Short-Term Memory」（LSTM）という新しいリカレントネットワークアーキテクチャと、適切な勾配ベースの学習アルゴリズムを紹介します。LSTMは、これらの誤差逆流の問題を克服するように設計されています。

ノイズの多い圧縮不可能な入力シーケンスにおいても、短い時間ラグの能力を失うことなく、1000ステップを超える時間間隔を橋渡しすることができます。

これは、特別なユニットの内部状態を通じて一定の（したがって、発散も消失もしない）誤差フローを強制するアーキテクチャのための効率的な勾配ベースのアルゴリズムによって達成されます（勾配計算が特定のアーキテクチャ特有のポイントで切り詰められる場合でも、長期的な誤差フローには影響しません）。

### Outline of paper

論文の概要。第2章では、これまでの研究を簡単にレビューします。第3章では、Hochreiter（1991）による誤差消失の詳細な分析の概要から始めます。

その後、教育目的のために定常誤差逆伝播への素朴なアプローチを紹介し、情報の保存と取得に関する問題点を強調します。

これらの問題が第4章で説明するLSTMアーキテクチャにつながります。

第5章では、多数の実験と競合する手法との比較を紹介します。

LSTMはそれらを上回り、他のリカレントネットアルゴリズムが解決できなかった複雑な人工的なタスクを解決することを学習します。

第6章では、LSTMの限界と利点について議論します。付録には、アルゴリズムの詳細な説明（A.1）と明示的な誤差フローの公式（A.2）が含まれています。


## 2 PREVIOUS WORK

このセクションでは、時間変化する入力を持つリカレントネット（固定入力および固定点に基づく勾配計算を持つネットとは対照的に、例：Almeida 1987, Pineda 1987）に焦点を当てます。

### 1.  radient-descent variants

Elman（1988）、Fahlman（1991）、Williams（1989）、Schmidhuber（1992a）、Pearlmutter（1989）のアプローチ、そしてPearlmutterの包括的な概観（1995）にある多くの関連アルゴリズムは、BPTTやRTRLと同じ問題に悩まされています（セクション1および3を参照）。

### 2. Time-delays
短い時間ラグに対して実用的と思われる他の方法は、タイムデレイニューラルネットワーク（Lang et al. 1990）およびPlateの方法（Plate 1993）であり、古いアクティベーションの重み付き合計に基づいてユニットのアクティベーションを更新します（de VriesおよびPrincipe 1991も参照）。Lin et al.（1995）は、タイムデレイネットワークの変種であるNARXネットワークを提案しています。

### 3. Time constants

長い時間ラグに対処するために、Mozer（1992）はユニットのアクティベーションの変化に影響を与えるタイムコンスタントを使用します（de VriesおよびPrincipeの前述のアプローチ（1991）は、実際にはTDNNとタイムコンスタントの混合と見なすことができます）。しかし、長い時間ラグの場合、タイムコンスタントには外部の細かい調整が必要です（Mozer 1992）。Sun et al.（1993）の代替アプローチは、古いアクティベーションと（スケーリングされた）現在のネット入力を加えることでリカレントユニットのアクティベーションを更新します。しかし、ネット入力は保存された情報を攪乱する傾向があり、長期記憶を実用的にしません。

### 4. Ring's approach

Ring's approach(1993) は長い時間ラグを橋渡しするための方法も提案しました。彼のネットワーク内のユニットが矛盾するエラー信号を受け取ると、適切な接続に影響を与える高次ユニットを追加します。彼のアプローチは時折非常に高速であることがありますが、100ステップの時間ラグを橋渡しするには100ユニットを追加する必要がある場合があります。また、リングのネットは見たことのないラグの期間に対して一般化しません。

### 5. Bengio et al.'s approaches

Bengio ら (1994) は、シミュレートアニーリング、多重グリッドランダム探索、時間加重擬似ニュートン最適化、および離散エラー伝播などの方法を調査しています。彼らの「ラッチ」および「2シーケンス」問題は、最小時間ラグ100の問題3aに非常に類似しています（実験3参照）。Bengio と Frasconi (1994) もターゲットを伝播するためのEMアプローチを提案しています。n個の「状態ネットワーク」と呼ばれるものがある場合、特定の時点でシステムはn種類の状態のいずれかにしか存在できません。セクション5の冒頭も参照してください。しかし、「加算問題」（セクション5.4）のような連続的な問題を解決するには、彼らのシステムには許容できない数の状態（つまり、状態ネットワーク）が必要です。

### 6. Kalman Filters

Puskorius と Feldkamp (1994) は、カルマンフィルタ技術を使用してリカレントネットの性能を向上させます。彼らは「過去の動的な導関数の効果を指数関数的に減衰させる導関数割引係数」を使用しているため、彼らのカルマンフィルタ訓練リカレントネットワークが非常に長い最小時間ラグに役立つとは考えにくいです。

### 7. Second order nets

LSTMが乗算ユニット（MUs）を使用して、不要な干渉から誤差フローを保護することを見ていきます。MUsを使用する最初のリカレントネット手法ではありません。例えば、WatrousとKuhn（1992）は、二次ネットにMUsを使用しています。LSTMとの違いは以下の通りです：(1) WatrousとKuhnのアーキテクチャは一定の誤差フローを強制せず、長い時間ラグの問題を解決するようには設計されていません。(2) それは完全に接続された二次シグマ-パイユニットを持ちますが、LSTMアーキテクチャのMUsは一定の誤差フローへのアクセスをゲートするためにのみ使用されます。(3) WatrousとKuhnのアルゴリズムは時間ステップごとにO(W^2)の操作を必要としますが、我々のものはO(W)しか必要としません。ここで、Wは重みの数です。MUsに関する追加の研究については、MillerとGiles（1993）も参照してください。

### 8. Simple weight guessing 

勾配ベースのアプローチの長い時間ラグ問題を回避するために、すべてのネットワーク重みをランダムに初期化し、結果として得られるネットがすべての訓練シーケンスを正しく分類するまで試行するという方法があります。実際、最近の発見（SchmidhuberとHochreiter 1996、HochreiterとSchmidhuber 1996、1997）によれば、単純な重みの推測が、(Bengio 1994、BengioとFrasconi 1994、MillerとGiles 1993、Linら 1995)の問題を、それらの提案されたアルゴリズムよりも速く解決することがわかりました。これは、重みの推測が優れたアルゴリズムであることを意味するわけではありません。ただし、問題が非常に簡単であることを意味します。より現実的なタスクでは、多くの自由パラメータ（例：入力重み）や高精度の重み（例：連続値パラメータ）が必要となり、推測が全く実行不可能になります。
  
### 9.Adaptive sequence chunkers 

Schmidhuberの階層的チャンクシステム（1992b、1993）は、任意の時間ラグを橋渡しする能力を持っていますが、これは時間ラグを引き起こす部分シーケンス間に局所的な予測可能性がある場合に限られます（Mozer 1992も参照）。例えば、Schmidhuberは彼のポスドク論文（1993）で、階層的リカレントネットを使用して、1000ステップを超える最小時間ラグを含む特定の文法学習タスクを迅速に解決しています。しかし、チャンクシステムのパフォーマンスは、ノイズレベルが増加し、入力シーケンスが圧縮しにくくなるにつれて劣化します。一方、LSTMはこの問題に悩まされません。

# 3 CONSTANT ERROR BACKPROP

## 3.1 Forward Pass

隠れユニットiのネット入力と活性化は次の通りです。
$$
\text{net}_i(t) = \sum_u w_{iu} y_u(t - 1) \tag{6}
$$
$$
y_i(t) = f_i(\text{net}_i(t))
$$

injのネット入力と活性化は次の通りです。
$$
\text{net}_{\text{inj}}(t) = \sum_u w_{\text{inj} \, u} y_u(t - 1) \tag{7}
$$
$$
y_{\text{inj}}(t) = f_{\text{inj}}(\text{net}_{\text{inj}}(t))
$$

outjのネット入力と活性化は次の通りです。
$$
\text{net}_{\text{out} \, j}(t) = \sum_u w_{\text{out} \, j \, u} y_u(t - 1) \tag{8}
$$
$$
y_{\text{out} \, j}(t) = f_{\text{out} \, j}(\text{net}_{\text{out} \, j}(t))
$$

メモリセルブロックcjのv番目のメモリセルのネット入力 $\text{net}_{c \, v \, j}$、内部状態 $\text{sc}_{v \, j}$、出力活性化 $y_{c \, v \, j}$は次の通りです：
$$
\text{net}_{c \, v \, j}(t) = \sum_u w_{c \, v \, j} y_u(t - 1) \tag{9}
$$
$$
\text{sc}_{v \, j}(t) = \text{sc}_{v \, j}(t - 1) + y_{\text{inj}}(t) g(\text{net}_{c \, v \, j}(t))
$$
$$
y_{c \, v \, j}(t) = y_{\text{out} \, j}(t) h(\text{sc}_{v \, j})
$$

出力ユニットkのネット入力と活性化は次の通りです。
$$
\text{net}_k(t) = \sum_u (u \text{ not a gate}) w_{ku} y_u(t - 1)
$$
$$
y_k(t) = f_k(\text{net}_k(t))
$$

後述するバックワードパス（逆伝播）は、以下のtruncated backprop（切り捨て逆伝播）に基づいています。

## 3.2 truncated backprop逆伝播の近似導関数

truncated backprop（第4章参照）は偏導関数を近似するだけであり、以下の表記にある$\delta_{\text{tr}}$記号で反映されています。誤差の流れがメモリセルやゲートユニットを離れた時点で切り捨てます。

切り捨ては、あるメモリセルを入力または入力ゲートを通じて離れた誤差が、出力または出力ゲートを通じて再びセルに入るループがないことを保証します。これにより、メモリ内での誤差の流れが一定であることが保証されます。


truncated版（セクション4参照）は、部分導関数を近似するだけであり、以下の表記に見られるように「tr」記号で示されています。これは、誤差フローがメモリセルやゲートユニットを離れた時点で切り詰めます。

切り詰めにより、あるメモリセルから入力や入力ゲートを通じて出た誤差が、そのセルの出力や出力ゲートを通じて再びセルに戻るループがないことが保証されます。

これにより、メモリセル内で一定の誤差フローが確保されます。

切り詰め逆伝播版では、以下の導関数はゼロに置き換えられます：

$
\frac{\partial \text{netin}_j (t)}{\partial y_u (t - 1)} \bigg|_{\text{tr}} = 0 \quad \forall u;
$

$
\frac{\partial \text{netout}_j (t)}{\partial y_u (t - 1)} \bigg|_{\text{tr}} = 0 \quad \forall u;
$

および

$
\frac{\partial \text{netc}_j (t)}{\partial y_u (t - 1)} \bigg|_{\text{tr}} = 0 \quad \forall u
$

したがって、次のようになります。
$$
\frac{\partial y_{\text{inj}}(t)}{\partial y_u(t - 1)} = f'_{\text{inj}}(\text{net}_{\text{inj}}(t)) \frac{\partial \text{net}_{\text{inj}}(t)}{\partial y_u(t - 1)} \delta_{\text{tr}}  {\approx}_{tr} 0 \quad \forall u;
$$
$$
\frac{\partial y_{\text{out} \, j}(t)}{\partial y_u(t - 1)} = f'_{\text{out} \, j}(\text{net}_{\text{out} \, j}(t)) \frac{\partial \text{net}_{\text{out} \, j}(t)}{\partial y_u(t - 1)} \delta_{\text{tr}} {\approx}_{tr} 0 \quad \forall u;
$$

そして、
$$
\frac{\partial y_{c \, j}(t)}{\partial y_u(t - 1)} =
\frac{\partial y_{c \, j}(t)}{\partial \text{net}_{\text{out} \, j}(t)} \frac{\partial \text{net}_{\text{out} \, j}(t)}{\partial y_u(t - 1)} +
\frac{\partial y_{c \, j}(t)}{\partial \text{net}_{\text{inj}}(t)} \frac{\partial \text{net}_{\text{inj}}(t)}{\partial y_u(t - 1)} +
\frac{\partial y_{c \, j}(t)}{\partial \text{net}_{c \, j}(t)} \frac{\partial \text{net}_{c \, j}(t)}{\partial y_u(t - 1)} {\approx}_{tr} 0 
$$

対応する $w_{jl}$ の総重み更新への寄与は $\eta \delta_j(t) y_l(t - 1)$ です。ここで、$\eta$ は学習率であり、$l$ はユニット $j$ に接続された任意のユニットを表します。


## 3.3 EXPONENTIALLY DECAYING ERROR
従来のBPTT（例：Williams and Zipser 1992）。時刻tにおける出力ユニットkのターゲットはdk(t)で表される。平均二乗誤差を使用する場合、kの誤差信号は次のようになる。

$$\delta_k(t) = f'_k(\text{net}_k(t))(d_k(t) - y_k(t));$$

ここで、

$$y_i(t) = f_i(\text{net}_i(t))$$

は、微分可能な活性化関数$f_i$を持つ非入力ユニットiの活性化であり、

$$\text{net}_i(t) = \sum_j w_{ij} y_j(t - 1)$$

はユニットiの現在のネット入力であり、$w_{ij}$はユニットjからiへの接続の重みである。非出力ユニットjの逆伝播誤差信号は次のようになる。

$$\delta_j(t) = f'_j(\text{net}_j(t)) \sum_i w_{ij} \delta_i(t + 1)$$

## 3.4 Outline of Hochreiter's analysis (1991, page 19-21)

入力ユニットを除くユニットのインデックスが1からnまでの完全に接続されたネットを持っているとします。

ユニットuからユニットvへの局所的な誤差の流れに焦点を当てます（後で、この分析が全体的な誤差の流れにすぐに拡張されることがわかります）。

時間ステップtで任意のユニットuで発生する誤差は、時間をqステップ戻して任意のユニットvに伝播されます。これは、以下の係数で誤差をスケールします：

$$
\frac{\partial \delta_v(t - q)}{\partial \delta_u(t)} =
\begin{cases}
f'_v(\text{net}_v(t - 1)) w_{uv} & \text{if } q = 1 \\
f'_v(\text{net}_v(t - q)) \sum_{l=1}^{n} \frac{\partial \delta_l(t - q + 1)}{\partial \delta_u(t)} w_{lv} & \text{if } q > 1
\end{cases}
$$

$l_q = v$ および $l_0 = u$ とすると、次のようになります：
$$
\frac{\partial \delta_v(t - q)}{\partial \delta_u(t)} = \sum_{l_1=1}^{n} \cdots \sum_{l_{q-1}=1}^{n} \prod_{m=1}^{q} f'_{l_m}(\text{net}_{l_m}(t - m)) w_{l_m l_{m-1}}
$$

（帰納法による証明）。$n^{q-1}$項の和 $\prod_{m=1}^{q} f'_{l_m}(\text{net}_{l_m}(t - m)) w_{l_m l_{m-1}}$ が総誤差の逆流を決定します（和の項が異なる符号を持つ可能性があるため、ユニットの数nを増やすことが必ずしも誤差の流れを増加させるわけではないことに注意してください）。

式（2）の直感的な説明。もし全てのmについて
$$
|f'_{l_m}(\text{net}_{l_m}(t - m)) w_{l_m l_{m-1}}| > 1.0
$$
である場合（例えば線形 $f_{l_m}$ の場合など）、最大積はqに対して指数関数的に増加します。つまり、誤差が吹き上がり、ユニットvに到達する相反する誤差信号が重みの振動と不安定な学習を引き起こす可能性があります（誤差の吹き上がりや分岐についてはPineda 1988、Baldi and Pineda 1991、Doya 1992も参照）。一方、もし全てのmについて
$$
|f'_{l_m}(\text{net}_{l_m}(t - m)) w_{l_m l_{m-1}}| < 1.0
$$
である場合、最大積はqに対して指数関数的に減少します。つまり、誤差が消滅し、許容できる時間内に学習ができなくなります。

## 6 DISCUSSION

### Limitations of LSTM.
- 1. LSTMアルゴリズムの特に効率的なトランケートバックプロパゲーションバージョンは、「強く遅延されたXOR問題」のような問題を容易に解決することはできません。この問題の目標は、ノイズの多いシーケンスのどこかで以前に発生した2つの広く離れた入力のXORを計算することです。その理由は、1つの入力だけを記憶しても予想誤差を減らすのに役立たないからです。このタスクは非分解可能であり、より簡単なサブゴールを最初に解決することによって誤差を段階的に減らすことが不可能です。

理論的には、完全な勾配を使用することでこの制限を回避することができます（おそらくメモリセルから入力を受け取る従来の隠れユニットを追加することで）。しかし、以下の理由から完全な勾配を計算することはお勧めしません：（1）計算の複雑さが増す。（2）CECを通る誤差の一定の流れはトランケートされたLSTMでのみ示すことができる。（3）実際に非トランケートLSTMでいくつかの実験を行いましたが、CECの外ではトランケートされたLSTMと比較して有意な差はありませんでした。CECの外では誤差の流れがすぐに消滅する傾向があるため、同じ理由で完全なBPTTはトランケートBPTTよりも優れていません。

- 2. 各メモリセルブロックには2つの追加ユニット（入力ゲートと出力ゲート）が必要です。しかし、標準的なリカレントネットと比較しても、重みの数が9倍以上に増えることはありません。LSTMアーキテクチャでは、従来の隠れユニットは最大で3つのユニットに置き換えられ、完全に接続された場合の重みの数は32倍に増加します。しかし、私たちの実験では、LSTMと競合するアプローチのアーキテクチャに対してかなり同等の重み数を使用しています。

- 3. 一般的に言えば、CECを通じてメモリセル内で誤差の流れが一定であるため、LSTMは全ての入力文字列を一度に見るフィードフォワードネットと似た問題に直面します。例えば、ランダムな重みの推測によってすぐに解決できるタスク（第5章の導入部分参照）、500ステップのパリティ問題などは、初期の重みが小さいトランケートLSTMアルゴリズムでは解決できません。この場合、LSTMの問題は、500入力のフィードフォワードネットが500ビットのパリティを解決しようとする問題と似ています。実際、LSTMは全入力を一度に見るバックプロパゲーションで訓練されたフィードフォワードネットのように振る舞うことがよくあります。

しかし、それがまさに、LSTMが多くの非自明なタスクにおいて、以前のアプローチを明確に上回る理由でもあります。
 
 - 4.LSTMは他のアプローチを超える「最近性」の概念に関する問題はありません。しかし、全ての勾配ベースのアプローチは、離散的な時間ステップを正確にカウントする実際の能力に欠けるという問題に苦しんでいます。もしある信号が99ステップ前に発生したか100ステップ前に発生したかが重要であるならば、追加のカウント機構が必要であるように思えます。しかし、例えば3ステップと11ステップの違いを認識するだけのような簡単なタスクは、LSTMにとって問題にはなりません。例えば、メモリセルの出力と入力の間に適切な負の接続を生成することで、LSTMは最近の入力により多くの重みを与え、必要な場合には減衰を学習することができます。

### Advantages of LSTM

メモリセル内の一定の誤差逆伝播により、LSTMは上記のような問題において非常に長い時間遅れを埋める能力を持ちます。

- この論文で議論されたような長い時間遅れの問題に対して、LSTMはノイズ、分散表現、および連続値を扱うことができます。有限状態オートマトンや隠れマルコフモデルとは対照的に、LSTMは事前に有限の状態数を選択する必要がありません。原理的には無限の状態数を扱うことができます。

- この論文で議論された問題に対して、LSTMは一般化が良好です。入力シーケンス内の広く離れた関連入力の位置が重要でなくても問題ありません。以前のアプローチとは異なり、適切な短い時間遅れの訓練例に依存せずに、LSTMは入力シーケンス内の特定の要素の二つ以上の広く離れた発生を迅速に区別することを学習します。

- パラメータの微調整は必要ないようです。LSTMは学習率、入力ゲートバイアス、出力ゲートバイアスなどの広範なパラメータ範囲でうまく機能します。例えば、一部の読者には我々の実験で使用された学習率が大きく感じられるかもしれません。しかし、大きな学習率は出力ゲートをゼロに向かって押し、結果的にそのネガティブな影響を自動的に打ち消します。

- LSTMアルゴリズムの重みと時間ステップごとの更新の複雑さは、本質的にBPTTと同じくO(1)です。これはRTRLのような他のアプローチと比較して優れています。しかし、完全なBPTTとは異なり、LSTMは空間的にも時間的にもローカルです。


## 7 CONCLUSION

各メモリセルの内部構造は、誤差逆伝播がメモリセルから漏れ出そうとするエラーフローを遮断する限り、一定のエラーカルーセル（CEC）内での一定のエラーフローを保証します。

これが非常に長い時間ラグを橋渡しするための基礎となります。

2つのゲートユニットが各メモリセルのCEC内のエラーフローへのアクセスを開閉することを学習します。

乗算入力ゲート(the multiplicative input gate)は、無関係な入力によるCECの干渉から保護します。同様に、乗算出力ゲート(the multiplicative output gate)は、現在無関係なメモリ内容による他のユニットへの干渉から保護します。

今後の課題として、LSTMの実用的な制限を明らかにするために、実際のデータに適用することを計画しています。

適用分野には、（1）時系列予測、（2）音楽作曲、（3）音声処理が含まれます。また、LSTMを使用してシーケンスチャンク（Schmidhuber 1992b, 1993）を強化し、両方の利点を組み合わせることも興味深いでしょう。